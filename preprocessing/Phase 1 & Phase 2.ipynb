{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "project_intro",
      "metadata": {
        "id": "project_intro"
      },
      "source": [
        "# Data Mining Project: Phase 1 and Phase 2\n",
        "\n",
        "our data mining project focused on building a search engine using the Cranfield dataset. Phase 1 involves preprocessing and indexing the dataset, while Phase 2 implements query processing with TF-IDF ranking."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "phase_1_intro",
      "metadata": {
        "id": "phase_1_intro"
      },
      "source": [
        "## Phase 1: Preprocessing and Indexing\n",
        "\n",
        "This section contains Phase 1 implementation, which loads the Cranfield dataset, preprocesses the titles (cleaning, tokenization, stemming), creates an inverted index using PyTerrier, and provides a basic search function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8t3D_oyXcxVf",
      "metadata": {
        "id": "8t3D_oyXcxVf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W4vf7dm1ecdR",
      "metadata": {
        "id": "W4vf7dm1ecdR"
      },
      "outputs": [],
      "source": [
        "input_file = \"cran.all.1400.csv\"\n",
        "output_file = \"cran_preprocessed_modern.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0S6unnGueh3S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S6unnGueh3S",
        "outputId": "4285d1f4-74f7-4f14-db03-9edbb67b7508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Loading the Cranfield Dataset ===\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1398 entries, 0 to 1397\n",
            "Data columns (total 5 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   Doc_NO          1398 non-null   int64 \n",
            " 1   Title           1398 non-null   object\n",
            " 2   Bib             1330 non-null   object\n",
            " 3   Text            1398 non-null   object\n",
            " 4   Processed_Text  1398 non-null   object\n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 54.7+ KB\n",
            "None\n",
            "\n",
            "First 5 rows of raw data:\n",
            "   Doc_NO                                              Title  \\\n",
            "0       1  experimental investigation of the aerodynamics...   \n",
            "1       2  simple shear flow past a flat plate in an inco...   \n",
            "2       3  the boundary layer in simple shear flow past a...   \n",
            "3       4  approximate solutions of the incompressible la...   \n",
            "4       5  one-dimensional transient heat conduction into...   \n",
            "\n",
            "                                                 Bib  \\\n",
            "0                         j. ae. scs. 25, 1958, 324.   \n",
            "1  department of aeronautical engineering, rensse...   \n",
            "2  department of mathematics, university of manch...   \n",
            "3                         j. ae. scs. 22, 1955, 728.   \n",
            "4                         j. ae. scs. 24, 1957, 924.   \n",
            "\n",
            "                                                Text  \\\n",
            "0  experimental investigation of the aerodynamics...   \n",
            "1  simple shear flow past a flat plate in an inco...   \n",
            "2  the boundary layer in simple shear flow past a...   \n",
            "3  approximate solutions of the incompressible la...   \n",
            "4  one-dimensional transient heat conduction into...   \n",
            "\n",
            "                                      Processed_Text  \n",
            "0  experiment investig of the aerodynam of a wing...  \n",
            "1  simpl shear flow past a flat plate in an incom...  \n",
            "2  the boundari layer in simpl shear flow past a ...  \n",
            "3  approxim solut of the incompress laminar bound...  \n",
            "4  onedimension transient heat conduct into a dou...  \n"
          ]
        }
      ],
      "source": [
        "print(\"=== Loading the Cranfield Dataset ===\")\n",
        "data = pd.read_csv(input_file)\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Dataset Info:\")\n",
        "print(df.info())\n",
        "print(\"\\nFirst 5 rows of raw data:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y_j6CerUejeG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_j6CerUejeG",
        "outputId": "99290e95-6982-4053-ef73-cda766045eca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Checking for Missing Values ===\n",
            "Missing values in 'Title': 0\n",
            "Missing values in 'Text': 0\n",
            "Total rows before dropping NaN: 1398\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Checking for Missing Values ===\")\n",
        "print(\"Missing values in 'Title':\", df['Title'].isna().sum())\n",
        "print(\"Missing values in 'Text':\", df['Text'].isna().sum())\n",
        "print(\"Total rows before dropping NaN:\", len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B8rJWu3flEus",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8rJWu3flEus",
        "outputId": "70e71535-a451-49ba-9826-28b00a88f700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total rows after dropping NaN in Title: 1398\n",
            "\n",
            "First 5 rows after dropping NaN:\n",
            "   Doc_NO                                              Title  \\\n",
            "0       1  experimental investigation of the aerodynamics...   \n",
            "1       2  simple shear flow past a flat plate in an inco...   \n",
            "2       3  the boundary layer in simple shear flow past a...   \n",
            "3       4  approximate solutions of the incompressible la...   \n",
            "4       5  one-dimensional transient heat conduction into...   \n",
            "\n",
            "                                                 Bib  \\\n",
            "0                         j. ae. scs. 25, 1958, 324.   \n",
            "1  department of aeronautical engineering, rensse...   \n",
            "2  department of mathematics, university of manch...   \n",
            "3                         j. ae. scs. 22, 1955, 728.   \n",
            "4                         j. ae. scs. 24, 1957, 924.   \n",
            "\n",
            "                                                Text  \\\n",
            "0  experimental investigation of the aerodynamics...   \n",
            "1  simple shear flow past a flat plate in an inco...   \n",
            "2  the boundary layer in simple shear flow past a...   \n",
            "3  approximate solutions of the incompressible la...   \n",
            "4  one-dimensional transient heat conduction into...   \n",
            "\n",
            "                                      Processed_Text  \n",
            "0  experiment investig of the aerodynam of a wing...  \n",
            "1  simpl shear flow past a flat plate in an incom...  \n",
            "2  the boundari layer in simpl shear flow past a ...  \n",
            "3  approxim solut of the incompress laminar bound...  \n",
            "4  onedimension transient heat conduct into a dou...  \n"
          ]
        }
      ],
      "source": [
        "df = df.dropna(subset=['Title'])\n",
        "print(\"Total rows after dropping NaN in Title:\", len(df))\n",
        "print(\"\\nFirst 5 rows after dropping NaN:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BpLbfv4gelRG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpLbfv4gelRG",
        "outputId": "4c4630a1-2192-45c7-e5a2-5f91f5b69484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Step 1: Cleaning Titles ===\n",
            "Sample of cleaned Titles (first 2 rows):\n",
            "   Doc_NO                                      Cleaned_Title\n",
            "0       1  experimental investigation of the aerodynamics...\n",
            "1       2  simple shear flow past a flat plate in an inco...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Step 1: Cleaning Titles ===\")\n",
        "cleaned_titles = []\n",
        "for title in df['Title']:\n",
        "\n",
        "    title_clean = re.sub(r'[^a-zA-Z\\s]', '', str(title))\n",
        "    title_clean = re.sub(r'\\s+', ' ', title_clean).strip()\n",
        "    cleaned_titles.append(title_clean.lower())\n",
        "df['Cleaned_Title'] = cleaned_titles\n",
        "print(\"Sample of cleaned Titles (first 2 rows):\")\n",
        "print(df[['Doc_NO', 'Cleaned_Title']].head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oMbMCVyLfAXv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMbMCVyLfAXv",
        "outputId": "e329a18f-b55d-4336-fa52-4c1a1fbc82c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Step 2: Tokenizing Titles and Vocabulary Analysis ===\n",
            "Total unique terms in Titles: 1804\n",
            "First 20 terms in Title vocabulary: ['ablating' 'ablation' 'accelerated' 'accelerating' 'according'\n",
            " 'accumulation' 'accuracy' 'acoustic' 'acoustical' 'acting' 'action'\n",
            " 'active' 'adapted' 'addendum' 'addition' 'adiabatic' 'adiabaticwall'\n",
            " 'adjacent' 'advances' 'advancing']\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Step 2: Tokenizing Titles and Vocabulary Analysis ===\")\n",
        "vectorizer = CountVectorizer(\n",
        "    stop_words=\"english\",\n",
        "    lowercase=True,\n",
        "    token_pattern=r'\\b[a-zA-Z]+\\b'\n",
        ")\n",
        "vector = vectorizer.fit_transform(df['Cleaned_Title'])\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "print(\"Total unique terms in Titles:\", len(terms))\n",
        "print(\"First 20 terms in Title vocabulary:\", terms[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S0vlxQ-RfDVp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0vlxQ-RfDVp",
        "outputId": "69e33a4e-6fe0-42e7-986d-c5d656f54ee6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample tokenized Titles (first 2 rows):\n",
            "   Doc_NO                                       Title_Tokens\n",
            "0       1  [experimental, investigation, of, the, aerodyn...\n",
            "1       2  [simple, shear, flow, past, a, flat, plate, in...\n"
          ]
        }
      ],
      "source": [
        "tokenized_titles = []\n",
        "for title in df['Cleaned_Title']:\n",
        "    words = title.split()\n",
        "    tokenized_titles.append(words)\n",
        "df['Title_Tokens'] = tokenized_titles\n",
        "print(\"\\nSample tokenized Titles (first 2 rows):\")\n",
        "print(df[['Doc_NO', 'Title_Tokens']].head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uMBGNCk4nH2t",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMBGNCk4nH2t",
        "outputId": "db630e3a-1329-4554-8b5d-0b2d9fd6c9de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Step 3: Comparing Stemming Methods ===\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Step 3: Comparing Stemming Methods ===\")\n",
        "porter = PorterStemmer()\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "lancaster = LancasterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ruCZVDS5fGla",
      "metadata": {
        "id": "ruCZVDS5fGla"
      },
      "outputs": [],
      "source": [
        "porter_stemmed = []\n",
        "snowball_stemmed = []\n",
        "lancaster_stemmed = []\n",
        "for word in terms:\n",
        "    porter_stemmed.append(porter.stem(word))\n",
        "    snowball_stemmed.append(snowball.stem(word))\n",
        "    lancaster_stemmed.append(lancaster.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kh4gOYRyi_YG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh4gOYRyi_YG",
        "outputId": "2bd1668e-0ec4-4fd2-bc72-fdb8db4ee7df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Stemming Comparison (First 5 Title Terms):\n",
            "------------------------------------------------------------\n",
            "Original        | Porter          | Snowball        | Lancaster      \n",
            "------------------------------------------------------------\n",
            "ablating        | ablat           | ablat           | abl            \n",
            "ablation        | ablat           | ablat           | abl            \n",
            "accelerated     | acceler         | acceler         | accel          \n",
            "accelerating    | acceler         | acceler         | accel          \n",
            "according       | accord          | accord          | accord         \n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nStemming Comparison (First 5 Title Terms):\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Original':<15} | {'Porter':<15} | {'Snowball':<15} | {'Lancaster':<15}\")\n",
        "print(\"-\" * 60)\n",
        "for i in range(min(5, len(terms))):\n",
        "    print(f\"{terms[i]:<15} | {porter_stemmed[i]:<15} | {snowball_stemmed[i]:<15} | {lancaster_stemmed[i]:<15}\")\n",
        "print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8Wawr023l1XQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wawr023l1XQ",
        "outputId": "ea484912-b2f2-48c5-e1ac-9b58e0f25094"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Applying Snowball Stemming to Title Tokens...\n",
            "Sample stemmed Titles (first 2 rows):\n",
            "   Doc_NO                               Stemmed_Title_Tokens\n",
            "0       1  [experiment, investig, of, the, aerodynam, of,...\n",
            "1       2  [simpl, shear, flow, past, a, flat, plate, in,...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nApplying Snowball Stemming to Title Tokens...\")\n",
        "stemmed_titles = []\n",
        "for tokens in df['Title_Tokens']:\n",
        "    stemmed_words = []\n",
        "    for word in tokens:\n",
        "        stemmed_words.append(snowball.stem(word))\n",
        "    stemmed_titles.append(stemmed_words)\n",
        "df['Stemmed_Title_Tokens'] = stemmed_titles\n",
        "print(\"Sample stemmed Titles (first 2 rows):\")\n",
        "print(df[['Doc_NO', 'Stemmed_Title_Tokens']].head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52VX7toJnS8k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52VX7toJnS8k",
        "outputId": "8511f677-835c-46cb-9c94-235d2395a3ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Step 4: Creating Processed_Text from Titles for Indexing ===\n",
            "Sample Processed_Text from Titles (first 2 rows):\n",
            "   Doc_NO                                     Processed_Text\n",
            "0       1  experiment investig of the aerodynam of a wing...\n",
            "1       2  simpl shear flow past a flat plate in an incom...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Step 4: Creating Processed_Text from Titles for Indexing ===\")\n",
        "processed_text = []\n",
        "for stemmed_tokens in df['Stemmed_Title_Tokens']:\n",
        "    joined = \" \".join(stemmed_tokens)\n",
        "    processed_text.append(joined)\n",
        "df['Processed_Text'] = processed_text\n",
        "print(\"Sample Processed_Text from Titles (first 2 rows):\")\n",
        "print(df[['Doc_NO', 'Processed_Text']].head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KjsrbwbPnXtm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjsrbwbPnXtm",
        "outputId": "066d4b5b-48f8-43c0-ef1d-4cc8f4b3ae4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Step 6: Saving Processed Data ===\n",
            "Saved to: cran_preprocessed_modern.csv\n",
            "Final output (first 5 rows):\n",
            "   Doc_NO                                              Title  \\\n",
            "0       1  experimental investigation of the aerodynamics...   \n",
            "1       2  simple shear flow past a flat plate in an inco...   \n",
            "2       3  the boundary layer in simple shear flow past a...   \n",
            "3       4  approximate solutions of the incompressible la...   \n",
            "4       5  one-dimensional transient heat conduction into...   \n",
            "\n",
            "                                                 Bib  \\\n",
            "0                         j. ae. scs. 25, 1958, 324.   \n",
            "1  department of aeronautical engineering, rensse...   \n",
            "2  department of mathematics, university of manch...   \n",
            "3                         j. ae. scs. 22, 1955, 728.   \n",
            "4                         j. ae. scs. 24, 1957, 924.   \n",
            "\n",
            "                                                Text  \\\n",
            "0  experimental investigation of the aerodynamics...   \n",
            "1  simple shear flow past a flat plate in an inco...   \n",
            "2  the boundary layer in simple shear flow past a...   \n",
            "3  approximate solutions of the incompressible la...   \n",
            "4  one-dimensional transient heat conduction into...   \n",
            "\n",
            "                                      Processed_Text  \n",
            "0  experiment investig of the aerodynam of a wing...  \n",
            "1  simpl shear flow past a flat plate in an incom...  \n",
            "2  the boundari layer in simpl shear flow past a ...  \n",
            "3  approxim solut of the incompress laminar bound...  \n",
            "4  onedimension transient heat conduct into a dou...  \n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Step 6: Saving Processed Data ===\")\n",
        "output_df = df[['Doc_NO', 'Title', 'Bib', 'Text', 'Processed_Text']]\n",
        "output_df.to_csv(output_file, index=False)\n",
        "print(\"Saved to:\", output_file)\n",
        "print(\"Final output (first 5 rows):\")\n",
        "print(output_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vw4dIjd8nVnu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw4dIjd8nVnu",
        "outputId": "d9a0749b-9ff3-4e40-c8b5-7398814456f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Step 5: Creative Title Insights ===\n",
            "Average token count per Title: 11.4\n",
            "Longest Title (tokens): 40 in Doc_NO: 1082\n",
            "Most frequent term in Titles (before stemming):\n",
            "'flow' appears 322 times\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Step 5: Creative Title Insights ===\")\n",
        "print(\"Average token count per Title:\", round(df['Title_Tokens'].apply(len).mean(), 2))\n",
        "print(\"Longest Title (tokens):\", df['Title_Tokens'].apply(len).max(), \"in Doc_NO:\",\n",
        "      df['Doc_NO'][df['Title_Tokens'].apply(len).idxmax()])\n",
        "print(\"Most frequent term in Titles (before stemming):\")\n",
        "word_counts = vector.toarray().sum(axis=0)\n",
        "top_term_idx = word_counts.argmax()\n",
        "print(f\"'{terms[top_term_idx]}' appears {word_counts[top_term_idx]} times\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nCN4_g7KflOl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCN4_g7KflOl",
        "outputId": "31f5496f-70f7-4943-f80c-d8bd5ab330cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-terrier\n",
            "  Downloading python_terrier-0.13.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from python-terrier) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from python-terrier) (2.2.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from python-terrier) (10.7.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from python-terrier) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from python-terrier) (2.32.3)\n",
            "Collecting ir-datasets>=0.3.2 (from python-terrier)\n",
            "  Downloading ir_datasets-0.5.10-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting wget (from python-terrier)\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyjnius>=1.4.2 (from python-terrier)\n",
            "  Downloading pyjnius-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.11/dist-packages (from python-terrier) (1.2.18)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from python-terrier) (1.15.2)\n",
            "Collecting ir-measures>=0.3.1 (from python-terrier)\n",
            "  Downloading ir_measures-0.3.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting pytrec-eval-terrier>=0.5.3 (from python-terrier)\n",
            "  Downloading pytrec_eval_terrier-0.5.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (984 bytes)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from python-terrier) (3.1.6)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from python-terrier) (0.14.4)\n",
            "Collecting dill (from python-terrier)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from python-terrier) (1.4.2)\n",
            "Collecting chest (from python-terrier)\n",
            "  Downloading chest-0.2.3.tar.gz (9.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lz4 (from python-terrier)\n",
            "  Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets>=0.3.2->python-terrier) (4.13.4)\n",
            "Collecting inscriptis>=2.2.0 (from ir-datasets>=0.3.2->python-terrier)\n",
            "  Downloading inscriptis-2.6.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.11/dist-packages (from ir-datasets>=0.3.2->python-terrier) (5.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets>=0.3.2->python-terrier) (6.0.2)\n",
            "Collecting trec-car-tools>=2.5.4 (from ir-datasets>=0.3.2->python-terrier)\n",
            "  Downloading trec_car_tools-2.6-py3-none-any.whl.metadata (640 bytes)\n",
            "Collecting warc3-wet>=0.2.3 (from ir-datasets>=0.3.2->python-terrier)\n",
            "  Downloading warc3_wet-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting warc3-wet-clueweb09>=0.2.5 (from ir-datasets>=0.3.2->python-terrier)\n",
            "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zlib-state>=0.1.3 (from ir-datasets>=0.3.2->python-terrier)\n",
            "  Downloading zlib_state-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting ijson>=3.1.3 (from ir-datasets>=0.3.2->python-terrier)\n",
            "  Downloading ijson-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting unlzw3>=0.2.1 (from ir-datasets>=0.3.2->python-terrier)\n",
            "  Downloading unlzw3-0.2.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: pyarrow>=16.1.0 in /usr/local/lib/python3.11/dist-packages (from ir-datasets>=0.3.2->python-terrier) (18.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->python-terrier) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->python-terrier) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->python-terrier) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->python-terrier) (2025.4.26)\n",
            "Collecting heapdict (from chest->python-terrier)\n",
            "  Downloading HeapDict-1.0.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated->python-terrier) (1.17.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->python-terrier) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->python-terrier) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->python-terrier) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->python-terrier) (2025.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->python-terrier) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels->python-terrier) (24.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets>=0.3.2->python-terrier) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets>=0.3.2->python-terrier) (4.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->python-terrier) (1.17.0)\n",
            "Collecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir-datasets>=0.3.2->python-terrier)\n",
            "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading python_terrier-0.13.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.4/163.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ir_datasets-0.5.10-py3-none-any.whl (859 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m859.0/859.0 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ir_measures-0.3.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyjnius-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytrec_eval_terrier-0.5.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.0/288.0 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ijson-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.2/119.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inscriptis-2.6.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
            "Downloading unlzw3-0.2.3-py3-none-any.whl (6.7 kB)\n",
            "Downloading warc3_wet-0.2.5-py3-none-any.whl (18 kB)\n",
            "Downloading zlib_state-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
            "Downloading HeapDict-1.0.1-py3-none-any.whl (3.9 kB)\n",
            "Building wheels for collected packages: chest, wget, warc3-wet-clueweb09, cbor\n",
            "  Building wheel for chest (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chest: filename=chest-0.2.3-py3-none-any.whl size=7611 sha256=4c90a110b072c1eee21ae4f1c409123924135a2533cd132e4fcfc8b0647d6496\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/57/13/28831e81239278141f874ee9b7d6d5490a1b1191c2d07a3e73\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=48dbf0dd046f0338610acc0980b9503b4d000baa63de1a2d7c02abcab3964954\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18919 sha256=58a36edf5a5330d2afbe97fd81aaf1650d1fc617c7b31f72805f0c02479d70cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/f9/dc/2dd16d3330e327236e4d407941975c42d5159d200cdb7922d8\n",
            "  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cbor: filename=cbor-1.0.0-cp311-cp311-linux_x86_64.whl size=53931 sha256=4e28c16515f6d6e1b920e7797cb8f33d76420167cc06e8540c7115e28b652e35\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/6b/45/0c34253b1af07d1d9dc524f6d44d74a6b191c43152e6aaf641\n",
            "Successfully built chest wget warc3-wet-clueweb09 cbor\n",
            "Installing collected packages: wget, warc3-wet-clueweb09, warc3-wet, pyjnius, ijson, heapdict, cbor, zlib-state, unlzw3, trec-car-tools, pytrec-eval-terrier, lz4, dill, chest, ir-measures, inscriptis, ir-datasets, python-terrier\n",
            "Successfully installed cbor-1.0.0 chest-0.2.3 dill-0.4.0 heapdict-1.0.1 ijson-3.3.0 inscriptis-2.6.0 ir-datasets-0.5.10 ir-measures-0.3.7 lz4-4.4.4 pyjnius-1.6.1 python-terrier-0.13.0 pytrec-eval-terrier-0.5.7 trec-car-tools-2.6 unlzw3-0.2.3 warc3-wet-0.2.5 warc3-wet-clueweb09-0.2.5 wget-3.2 zlib-state-0.1.9\n"
          ]
        }
      ],
      "source": [
        "!pip install python-terrier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1r2h34lcfoKl",
      "metadata": {
        "id": "1r2h34lcfoKl"
      },
      "outputs": [],
      "source": [
        "import pyterrier as pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zrns3BJMfue4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zrns3BJMfue4",
        "outputId": "5fe7e252-b1ed-4f6b-9695-a38e7a4d122d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "terrier-assemblies 5.11 jar-with-dependencies not found, downloading to /root/.pyterrier...\n",
            "Done\n",
            "terrier-python-helper 0.0.8 jar not found, downloading to /root/.pyterrier...\n",
            "Done\n",
            "Java Virtual Machine started!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Java started and loaded: pyterrier.java, pyterrier.terrier.java [version=5.11 (build: craig.macdonald 2025-01-13 21:29), helper_version=0.0.8]\n"
          ]
        }
      ],
      "source": [
        "if not pt.java.started():\n",
        "    pt.java.init()\n",
        "    print(\"Java Virtual Machine started!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WN011XDZfx1a",
      "metadata": {
        "id": "WN011XDZfx1a"
      },
      "outputs": [],
      "source": [
        "input_file = \"/content/cran_preprocessed_modern.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dC4N9G_qo2kg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC4N9G_qo2kg",
        "outputId": "60e12b5e-cd7d-46a7-94bb-e48b39b4a532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Doc_NO                                              Title  \\\n",
            "0       1  experimental investigation of the aerodynamics...   \n",
            "1       2  simple shear flow past a flat plate in an inco...   \n",
            "2       3  the boundary layer in simple shear flow past a...   \n",
            "3       4  approximate solutions of the incompressible la...   \n",
            "4       5  one-dimensional transient heat conduction into...   \n",
            "\n",
            "                                                 Bib  \\\n",
            "0                         j. ae. scs. 25, 1958, 324.   \n",
            "1  department of aeronautical engineering, rensse...   \n",
            "2  department of mathematics, university of manch...   \n",
            "3                         j. ae. scs. 22, 1955, 728.   \n",
            "4                         j. ae. scs. 24, 1957, 924.   \n",
            "\n",
            "                                                Text  \\\n",
            "0  experimental investigation of the aerodynamics...   \n",
            "1  simple shear flow past a flat plate in an inco...   \n",
            "2  the boundary layer in simple shear flow past a...   \n",
            "3  approximate solutions of the incompressible la...   \n",
            "4  one-dimensional transient heat conduction into...   \n",
            "\n",
            "                                      Processed_Text  \n",
            "0  experiment investig of the aerodynam of a wing...  \n",
            "1  simpl shear flow past a flat plate in an incom...  \n",
            "2  the boundari layer in simpl shear flow past a ...  \n",
            "3  approxim solut of the incompress laminar bound...  \n",
            "4  onedimension transient heat conduct into a dou...  \n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(input_file)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nzKiw_9ypAbD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzKiw_9ypAbD",
        "outputId": "18a1b09a-fbdd-4c56-84f5-1871f1661926"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample with docno (first 2 rows):\n",
            "  docno                                              Title  \\\n",
            "0     1  experimental investigation of the aerodynamics...   \n",
            "1     2  simple shear flow past a flat plate in an inco...   \n",
            "\n",
            "                                      Processed_Text  \n",
            "0  experiment investig of the aerodynam of a wing...  \n",
            "1  simpl shear flow past a flat plate in an incom...  \n"
          ]
        }
      ],
      "source": [
        "df[\"docno\"] = df[\"Doc_NO\"].astype(str)\n",
        "print(\"\\nSample with docno (first 2 rows):\")\n",
        "print(df[['docno', 'Title', 'Processed_Text']].head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qrdFGYFapFQ7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrdFGYFapFQ7",
        "outputId": "687d3c06-f0c4-4c29-c0f9-f257e316f56d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-a1ca258c94dc>:3: DeprecationWarning: Call to deprecated class DFIndexer. (use pt.terrier.IterDictIndexer().index(dataframe.to_dict(orient='records')) instead) -- Deprecated since version 0.11.0.\n",
            "  indexer = pt.DFIndexer(\"./CranfieldTitleIndex\", overwrite=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Step 1: Creating and Indexing the Titles ===\n",
            "Index location: ./CranfieldTitleIndex/data.properties\n",
            "Indexing complete! Stored at: ./CranfieldTitleIndex/data.properties\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Step 1: Creating and Indexing the Titles ===\")\n",
        "\n",
        "indexer = pt.DFIndexer(\"./CranfieldTitleIndex\", overwrite=True)\n",
        "index_ref = indexer.index(df[\"Processed_Text\"], df[\"docno\"])\n",
        "print(\"Index location:\", index_ref.toString())\n",
        "print(\"Indexing complete! Stored at:\", index_ref.toString())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dUi1YzJwpdyp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUi1YzJwpdyp",
        "outputId": "8545b6b0-c4a2-4c75-f8bd-4da10548f107"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Step 2: Loading the Index ===\n",
            "Index loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Step 2: Loading the Index ===\")\n",
        "\n",
        "index = pt.IndexFactory.of(index_ref)\n",
        "print(\"Index loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3-KXC9Kopg2w",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-KXC9Kopg2w",
        "outputId": "49db218e-2bf8-45c9-e0ad-9753d466593e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ablat -> Nt=12 TF=12 maxTF=1\n",
            "accel -> Nt=2 TF=2 maxTF=1\n",
            "accord -> Nt=1 TF=1 maxTF=1\n",
            "accumul -> Nt=1 TF=1 maxTF=1\n",
            "accuraci -> Nt=2 TF=2 maxTF=1\n",
            "acoust -> Nt=5 TF=5 maxTF=1\n",
            "act -> Nt=1 TF=1 maxTF=1\n",
            "action -> Nt=1 TF=1 maxTF=1\n",
            "activ -> Nt=1 TF=1 maxTF=1\n",
            "adapt -> Nt=1 TF=1 maxTF=1\n"
          ]
        }
      ],
      "source": [
        "lexicon = index.getLexicon()\n",
        "\n",
        "count = 0\n",
        "\n",
        "\n",
        "for kv in lexicon:\n",
        "    if count < 10:\n",
        "        term = kv.getKey()\n",
        "        entry = kv.getValue()\n",
        "        print(f\"{term} -> Nt={entry.getNumberOfEntries()} TF={entry.getFrequency()} maxTF={entry.getMaxFrequencyInDocuments()}\")\n",
        "        count = count + 1\n",
        "    else:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WJKt0WmLqA3V",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJKt0WmLqA3V",
        "outputId": "57ed0fc0-3daf-4e1c-fe20-37d769328d7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Step 5: Setting Up Search Function ===\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Step 5: Setting Up Search Function ===\")\n",
        "def search_term(term):\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    term = term.lower()\n",
        "    stemmed_term = stemmer.stem(term)\n",
        "\n",
        "    print(f\"\\nSearching for: '{term}' (stemmed: '{stemmed_term}')\")\n",
        "\n",
        "    try:\n",
        "        pointer = index.getLexicon()[stemmed_term]\n",
        "        print(f\"Found term '{stemmed_term}' with stats: {pointer.toString()}\")\n",
        "        print(\"Documents containing the term:\")\n",
        "        postings = index.getInvertedIndex().getPostings(pointer)\n",
        "\n",
        "\n",
        "        for posting in postings:\n",
        "            doc_id = posting.getId()\n",
        "            doc_length = posting.getDocumentLength()\n",
        "            print(f\"- Doc ID: {doc_id} (docno: {df['docno'].iloc[doc_id]}), Length: {doc_length}\")\n",
        "    except KeyError:\n",
        "        print(f\"Term '{stemmed_term}' not found in the index.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oXcg_IEGrIKx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXcg_IEGrIKx",
        "outputId": "a3aed864-65d3-4e7e-eef9-e3706680286c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Searching for: 'information' (stemmed: 'inform')\n",
            "Found term 'inform' with stats: term700 Nt=1 TF=1 maxTF=1 @{0 5628 7}\n",
            "Documents containing the term:\n",
            "- Doc ID: 439 (docno: 440), Length: 8\n",
            "\n",
            "Searching for: 'omar' (stemmed: 'omar')\n",
            "Term 'omar' not found in the index.\n"
          ]
        }
      ],
      "source": [
        "search_term(\"information\")\n",
        "search_term(\"Omar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "phase_2_intro",
      "metadata": {
        "id": "phase_2_intro"
      },
      "source": [
        "## Phase 2: Query Processing with TF-IDF Ranking\n",
        "\n",
        "This section implements Phase 2 of the project, focusing on query processing with expanded capabilities. It includes parsing user queries, applying the same preprocessing steps as in Phase 1 (tokenization, lowercase, stemming), retrieving documents containing all query terms using the inverted index, and ranking them using TF-IDF scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "phase_2_import_libraries",
      "metadata": {
        "id": "phase_2_import_libraries"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "preprocess_query",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "preprocess_query",
        "outputId": "2cfb3345-f43b-49f4-8dc6-6eaa5f7d1317"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample query: Experimental Aerodynamics Wing\n",
            "Preprocessed query tokens: ['experiment', 'aerodynam', 'wing']\n"
          ]
        }
      ],
      "source": [
        "def preprocess_query(query, stemmer=SnowballStemmer('english')):\n",
        "    query = query.lower()\n",
        "    query = re.sub(r'[^a-zA-Z\\s]', '', query)\n",
        "    query = re.sub(r'\\s+', ' ', query).strip()\n",
        "    tokens = query.split()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return stemmed_tokens\n",
        "\n",
        "\n",
        "sample_query = 'Experimental Aerodynamics Wing'\n",
        "print('Sample query:', sample_query)\n",
        "print('Preprocessed query tokens:', preprocess_query(sample_query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "retrieve_documents",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "retrieve_documents",
        "outputId": "8a9471b0-2784-4056-9c0d-926e2db557ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Documents retrieved for query \"aerodynamics wing\":\n",
            "Docno: 1, Title: experimental investigation of the aerodynamics of a\n",
            "      wing in a slipstream .\n",
            "Docno: 708, Title: aerodynamic characteristics of two winged reentry vehicles at supersonic\n",
            "    and hypersonic speeds .\n"
          ]
        }
      ],
      "source": [
        "def retrieve_documents(query_tokens, index, df):\n",
        "    lexicon = index.getLexicon()\n",
        "    doc_sets = []\n",
        "\n",
        "    for token in query_tokens:\n",
        "        try:\n",
        "            pointer = lexicon[token]\n",
        "            postings = index.getInvertedIndex().getPostings(pointer)\n",
        "            doc_ids = [posting.getId() for posting in postings]\n",
        "            doc_sets.append(set(doc_ids))\n",
        "        except KeyError:\n",
        "            print(f\"Term '{token}' not found in index.\")\n",
        "            return []\n",
        "\n",
        "    if not doc_sets:\n",
        "        return []\n",
        "    common_docs = list(set.intersection(*doc_sets))\n",
        "\n",
        "    results = []\n",
        "    for doc_id in common_docs:\n",
        "        docno = df['docno'].iloc[doc_id]\n",
        "        title = df['Title'].iloc[doc_id]\n",
        "        processed_text = df['Processed_Text'].iloc[doc_id]\n",
        "        results.append({\n",
        "            'doc_id': doc_id,\n",
        "            'docno': docno,\n",
        "            'title': title,\n",
        "            'processed_text': processed_text\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "test_query = 'aerodynamics wing'\n",
        "test_tokens = preprocess_query(test_query)\n",
        "docs = retrieve_documents(test_tokens, index, df)\n",
        "print(f'\\nDocuments retrieved for query \"{test_query}\":')\n",
        "for doc in docs[:2]:\n",
        "    print(f\"Docno: {doc['docno']}, Title: {doc['title']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rank_documents",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rank_documents",
        "outputId": "4a3fc661-6e11-4871-9219-fcd71d086029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top ranked documents for query \"aerodynamics wing\":\n",
            "Docno: 1, Title: experimental investigation of the aerodynamics of a\r\n",
            "      wing in a slipstream ., TF-IDF Score: 1.4142\n",
            "Docno: 708, Title: aerodynamic characteristics of two winged reentry vehicles at supersonic\r\n",
            "    and hypersonic speeds ., TF-IDF Score: 1.4142\n"
          ]
        }
      ],
      "source": [
        "def rank_documents(documents, query_tokens):\n",
        "    if not documents:\n",
        "        return []\n",
        "\n",
        "    corpus = [doc['processed_text'] for doc in documents]\n",
        "    query = ' '.join(query_tokens)\n",
        "\n",
        "    vectorizer = TfidfVectorizer(vocabulary=query_tokens)\n",
        "    try:\n",
        "        tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "        scores = tfidf_matrix.sum(axis=1).A1\n",
        "    except ValueError as e:\n",
        "        print('TF-IDF calculation failed:', e)\n",
        "        scores = [0] * len(documents)\n",
        "\n",
        "    for i, doc in enumerate(documents):\n",
        "        doc['tfidf_score'] = scores[i]\n",
        "\n",
        "    ranked_docs = sorted(documents, key=lambda x: x['tfidf_score'], reverse=True)\n",
        "    return ranked_docs\n",
        "\n",
        "ranked_docs = rank_documents(docs, test_tokens)\n",
        "print(f'\\nTop ranked documents for query \"{test_query}\":')\n",
        "for doc in ranked_docs[:2]:\n",
        "    print(f\"Docno: {doc['docno']}, Title: {doc['title']}, TF-IDF Score: {doc['tfidf_score']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "search_function",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "search_function",
        "outputId": "08b4aefb-1d53-4ddf-f2f8-37a9860976f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Searching for: \"aerodynamics wing\" ===\n",
            "Query tokens: ['aerodynam', 'wing']\n",
            "Found 17 documents.\n",
            "Top 5 results:\n",
            "1. Docno: 1, Title: experimental investigation of the aerodynamics of a\n",
            "      wing in a slipstream ., TF-IDF Score: 1.4142\n",
            "2. Docno: 708, Title: aerodynamic characteristics of two winged reentry vehicles at supersonic\n",
            "    and hypersonic speeds ., TF-IDF Score: 1.4142\n",
            "3. Docno: 1289, Title: numerical technique to lifting surface theory for calculation\n",
            "    of unsteady aerodynamic forces due to continuous sinusoidal\n",
            "    gusts on several wing planforms at sobsonic speeds ., TF-IDF Score: 1.4142\n",
            "4. Docno: 712, Title: low-speed longitudinal aerodynamic characteristics associated with a\n",
            "    series of low-aspect ratio wings having variations in leading-edge\n",
            "    contour ., TF-IDF Score: 1.4142\n",
            "5. Docno: 1164, Title: effect of ground proximity on the aerodynamic characteristics\n",
            "    of a four- engined vertical take-off and landing transport\n",
            "    airplane model with tilting wing and propellers ., TF-IDF Score: 1.4142\n",
            "\n",
            "=== Searching for: \"information retrieval\" ===\n",
            "Query tokens: ['inform', 'retriev']\n",
            "Term 'retriev' not found in index.\n",
            "No documents found.\n",
            "\n",
            "=== Searching for: \"nonexistent term\" ===\n",
            "Query tokens: ['nonexist', 'term']\n",
            "Term 'nonexist' not found in index.\n",
            "No documents found.\n"
          ]
        }
      ],
      "source": [
        "def search(query, index, df, top_k=5):\n",
        "    print(f'\\n=== Searching for: \"{query}\" ===')\n",
        "    query_tokens = preprocess_query(query)\n",
        "    print('Query tokens:', query_tokens)\n",
        "\n",
        "    documents = retrieve_documents(query_tokens, index, df)\n",
        "    if not documents:\n",
        "        print('No documents found.')\n",
        "        return\n",
        "    print(f'Found {len(documents)} documents.')\n",
        "\n",
        "    ranked_docs = rank_documents(documents, query_tokens)\n",
        "\n",
        "    print(f'Top {min(top_k, len(ranked_docs))} results:')\n",
        "    for i, doc in enumerate(ranked_docs[:top_k], 1):\n",
        "        print(f'{i}. Docno: {doc[\"docno\"]}, Title: {doc[\"title\"]}, TF-IDF Score: {doc[\"tfidf_score\"]:.4f}')\n",
        "\n",
        "search('aerodynamics wing', index, df)\n",
        "search('information retrieval', index, df)\n",
        "search('nonexistent term', index, df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
