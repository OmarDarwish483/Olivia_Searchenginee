{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "363682e4",
   "metadata": {},
   "source": [
    "For Regular Indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ab96bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "044857ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = r\"D:\\DownLoad\\projects\\Search Engine\\Olivia_Searchengine\\datacollection\\output\\cran_preprocessed_modern.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6f89bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "69f0a27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_NO</th>\n",
       "      <th>Title</th>\n",
       "      <th>Bib</th>\n",
       "      <th>Text</th>\n",
       "      <th>Processed_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>experimental investigation of the aerodynamics...</td>\n",
       "      <td>j. ae. scs. 25, 1958, 324.</td>\n",
       "      <td>experimental investigation of the aerodynamics...</td>\n",
       "      <td>experiment investig of the aerodynam of a wing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>simple shear flow past a flat plate in an inco...</td>\n",
       "      <td>department of aeronautical engineering, rensse...</td>\n",
       "      <td>simple shear flow past a flat plate in an inco...</td>\n",
       "      <td>simpl shear flow past a flat plate in an incom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>the boundary layer in simple shear flow past a...</td>\n",
       "      <td>department of mathematics, university of manch...</td>\n",
       "      <td>the boundary layer in simple shear flow past a...</td>\n",
       "      <td>the boundari layer in simpl shear flow past a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>approximate solutions of the incompressible la...</td>\n",
       "      <td>j. ae. scs. 22, 1955, 728.</td>\n",
       "      <td>approximate solutions of the incompressible la...</td>\n",
       "      <td>approxim solut of the incompress laminar bound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>one-dimensional transient heat conduction into...</td>\n",
       "      <td>j. ae. scs. 24, 1957, 924.</td>\n",
       "      <td>one-dimensional transient heat conduction into...</td>\n",
       "      <td>onedimension transient heat conduct into a dou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Doc_NO                                              Title  \\\n",
       "0       1  experimental investigation of the aerodynamics...   \n",
       "1       2  simple shear flow past a flat plate in an inco...   \n",
       "2       3  the boundary layer in simple shear flow past a...   \n",
       "3       4  approximate solutions of the incompressible la...   \n",
       "4       5  one-dimensional transient heat conduction into...   \n",
       "\n",
       "                                                 Bib  \\\n",
       "0                         j. ae. scs. 25, 1958, 324.   \n",
       "1  department of aeronautical engineering, rensse...   \n",
       "2  department of mathematics, university of manch...   \n",
       "3                         j. ae. scs. 22, 1955, 728.   \n",
       "4                         j. ae. scs. 24, 1957, 924.   \n",
       "\n",
       "                                                Text  \\\n",
       "0  experimental investigation of the aerodynamics...   \n",
       "1  simple shear flow past a flat plate in an inco...   \n",
       "2  the boundary layer in simple shear flow past a...   \n",
       "3  approximate solutions of the incompressible la...   \n",
       "4  one-dimensional transient heat conduction into...   \n",
       "\n",
       "                                      Processed_Text  \n",
       "0  experiment investig of the aerodynam of a wing...  \n",
       "1  simpl shear flow past a flat plate in an incom...  \n",
       "2  the boundari layer in simpl shear flow past a ...  \n",
       "3  approxim solut of the incompress laminar bound...  \n",
       "4  onedimension transient heat conduct into a dou...  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "85f02fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "import os\n",
    "\n",
    "def Regular_indexing():\n",
    "    if not pt.started():\n",
    "        pt.init()\n",
    "        print(\"Java Virtual Machine started!\")\n",
    "\n",
    "    input_file = r\"D:\\DownLoad\\projects\\Search Engine\\Olivia_Searchengine\\datacollection\\output\\cran_preprocessed_modern.csv\"\n",
    "    df = pd.read_csv(input_file)\n",
    "    df[\"docno\"] = df[\"Doc_NO\"].astype(str)\n",
    "\n",
    "    # Validation checks\n",
    "    assert df[\"Processed_Text\"].notnull().all(), \"Processed_Text has null values!\"\n",
    "    assert df[\"docno\"].notnull().all(), \"docno has null values!\"\n",
    "    assert df[\"docno\"].is_unique, \"docno values are not unique!\"\n",
    "\n",
    "    index_path = os.path.abspath(\"./CranfieldTitleIndex\")\n",
    "    if not os.path.exists(index_path) or not os.listdir(index_path):\n",
    "        print(\"\\nIndexing documents...\")\n",
    "        indexer = pt.DFIndexer(index_path, overwrite=True)\n",
    "        index_ref = indexer.index(df[\"Processed_Text\"], df[\"docno\"])\n",
    "        print(\"Index created at:\", index_ref.toString())\n",
    "    else:\n",
    "        print(\"Index already exists at:\", index_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ba944a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def search_term_regular_indexing(query_term):\n",
    "    index_path = os.path.abspath(\"./CranfieldTitleIndex\")\n",
    "    if not os.path.exists(index_path) or not os.listdir(index_path):\n",
    "        print(\"Index not found. Run Regular_indexing() first.\")\n",
    "        return\n",
    "\n",
    "    index = pt.IndexFactory.of(index_path)\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    for term in query_term :\n",
    "        term = term.lower()\n",
    "        stemmed_term = stemmer.stem(term)\n",
    "\n",
    "        print(f\"\\nSearching for: '{term}' (stemmed: '{stemmed_term}')\")\n",
    "\n",
    "        try:\n",
    "            lexicon = index.getLexicon()\n",
    "            if stemmed_term not in lexicon:\n",
    "                print(f\"Term '{stemmed_term}' not found in the index.\")\n",
    "                return\n",
    "\n",
    "            pointer = lexicon[stemmed_term]\n",
    "            print(f\"Found term '{stemmed_term}' with stats: {pointer.toString()}\")\n",
    "\n",
    "            postings = index.getInvertedIndex().getPostings(pointer)\n",
    "            meta = index.getMetaIndex()\n",
    "\n",
    "            print(\"Documents containing the term:\")\n",
    "            for posting in postings:\n",
    "                doc_id = posting.getId()\n",
    "                docno = meta.getItem(\"docno\", doc_id)\n",
    "                doc_length = posting.getDocumentLength()\n",
    "                print(f\"- Doc ID: {doc_id} (docno: {docno}), Length: {doc_length}\")\n",
    "        except Exception as e:\n",
    "            print(\"Search failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ccb875",
   "metadata": {},
   "source": [
    "Adding Boolean Retreivel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b7087c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_query(query_terms, operator):\n",
    "    df=pd.read_csv(r\"D:\\DownLoad\\projects\\Search Engine\\Olivia_Searchengine\\datacollection\\output\\cran_preprocessed_modern.csv\")\n",
    "    # Convert all query terms to lowercase\n",
    "    query_terms = [term.lower() for term in query_terms]\n",
    "\n",
    "    # Use CountVectorizer to get binary term-document matrix\n",
    "    vectorizer = CountVectorizer(binary=True, stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform(df['Title'])\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    td_matrix = X.toarray()\n",
    "\n",
    "    # Create a query vector\n",
    "    query_vector = [1 if term in query_terms else 0 for term in terms]\n",
    "\n",
    "    result = td_matrix @ query_vector  # Matrix multiplication (dot product)\n",
    "\n",
    "    if operator.upper() == \"AND\":\n",
    "        return df['Doc_NO'][[count == len(query_terms) for count in result]].tolist()\n",
    "    elif operator.upper() == \"OR\":\n",
    "        return df['Doc_NO'][[count > 0 for count in result]].tolist()\n",
    "    elif operator.upper() == \"NOT\":\n",
    "        return df['Doc_NO'][[count == 0 for count in result]].tolist()\n",
    "    else:\n",
    "        raise ValueError(\"Operator must be 'AND', 'OR', or 'NOT'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "60c1d3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND: [1, 84, 189, 372, 423, 497, 569, 662, 766, 816, 836, 858, 1062, 1074, 1075, 1098, 1156, 1159, 1364]\n"
     ]
    }
   ],
   "source": [
    "print(\"AND:\", boolean_query( [\"experimental\", \"investigation\"], \"AND\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1183e910",
   "metadata": {},
   "source": [
    "Adding Query preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1b6626d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample query: Experimental Aerodynamics Wing\n",
      "Preprocessed query tokens: ['experiment', 'aerodynam', 'wing']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_query(query, stemmer=SnowballStemmer('english')):\n",
    "    query = query.lower()\n",
    "    query = re.sub(r'[^a-zA-Z\\s]', '', query)\n",
    "    query = re.sub(r'\\s+', ' ', query).strip()\n",
    "    tokens = query.split()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "\n",
    "sample_query = 'Experimental Aerodynamics Wing'\n",
    "print('Sample query:', sample_query)\n",
    "print('Preprocessed query tokens:', preprocess_query(sample_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d7eb6b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term 'experiment' not found in index.\n",
      "\n",
      "Documents retrieved for query \"experimental investigation\":\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def retrieve_documents(query_tokens, df):\n",
    "    index_path = os.path.abspath(\"./CranfieldTitleIndex\")\n",
    "    if not os.path.exists(index_path) or not os.listdir(index_path):\n",
    "        print(\"Index not found. Run Regular_indexing() first.\")\n",
    "        return\n",
    "\n",
    "    index = pt.IndexFactory.of(index_path)\n",
    "    lexicon = index.getLexicon()\n",
    "    doc_sets = []\n",
    "\n",
    "    for token in query_tokens:\n",
    "        try:\n",
    "            pointer = lexicon[token]\n",
    "            postings = index.getInvertedIndex().getPostings(pointer)\n",
    "            doc_ids = [posting.getId() for posting in postings]\n",
    "            doc_sets.append(set(doc_ids))\n",
    "        except KeyError:\n",
    "            print(f\"Term '{token}' not found in index.\")\n",
    "            return []\n",
    "\n",
    "    if not doc_sets:\n",
    "        return []\n",
    "    common_docs = list(set.intersection(*doc_sets))\n",
    "\n",
    "    results = []\n",
    "    for doc_id in common_docs:\n",
    "        docno = df['Doc_NO'].iloc[doc_id]\n",
    "        title = df['Title'].iloc[doc_id]\n",
    "        processed_text = df['Processed_Text'].iloc[doc_id]\n",
    "        results.append({\n",
    "            'doc_id': doc_id,\n",
    "            'docno': docno,\n",
    "            'title': title,\n",
    "            'processed_text': processed_text\n",
    "        })\n",
    "    for doc in results[:2]:\n",
    "        print(f\"Docno: {doc['docno']}, Title: {doc['title']}\")\n",
    "\n",
    "    \n",
    "\n",
    "test_query = 'experimental investigation' #aerodynamics wing\n",
    "test_tokens = preprocess_query(test_query)\n",
    "docs = retrieve_documents(test_tokens, df)\n",
    "print(f'\\nDocuments retrieved for query \"{test_query}\":')\n",
    "# for doc in docs[:2]:\n",
    "#     print(f\"Docno: {doc['docno']}, Title: {doc['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0c1a17bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top ranked documents for query \"experimental investigation\":\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def rank_documents(documents, query_tokens):\n",
    "    if not documents:\n",
    "        return []\n",
    "\n",
    "    corpus = [doc['processed_text'] for doc in documents]\n",
    "    query = ' '.join(query_tokens)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(vocabulary=query_tokens)\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "        scores = tfidf_matrix.sum(axis=1).A1\n",
    "    except ValueError as e:\n",
    "        print('TF-IDF calculation failed:', e)\n",
    "        scores = [0] * len(documents)\n",
    "\n",
    "    for i, doc in enumerate(documents):\n",
    "        doc['tfidf_score'] = scores[i]\n",
    "\n",
    "    ranked_docs = sorted(documents, key=lambda x: x['tfidf_score'], reverse=True)\n",
    "    return ranked_docs\n",
    "\n",
    "ranked_docs = rank_documents(docs, test_tokens)\n",
    "print(f'\\nTop ranked documents for query \"{test_query}\":')\n",
    "for doc in ranked_docs[:2]:\n",
    "    print(f\"Docno: {doc['docno']}, Title: {doc['title']}, TF-IDF Score: {doc['tfidf_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7935ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_search(query, df, top_k=5):\n",
    "    query = query.lower().strip()\n",
    "    tokens = re.findall(r'\\b\\w+\\b|and|or|not', query)\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    index_path = os.path.abspath(\"./CranfieldTitleIndex\")\n",
    "    if not os.path.exists(index_path) or not os.listdir(index_path):\n",
    "        print(\"Index not found. Run Regular_indexing() first.\")\n",
    "        return\n",
    "\n",
    "    index = pt.IndexFactory.of(index_path)\n",
    "    \n",
    "    lexicon = index.getLexicon()\n",
    "    doc_sets = []\n",
    "    current_term = ''\n",
    "    operator = 'AND'\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in ['and', 'or', 'not']:\n",
    "            operator = token.upper()\n",
    "            continue\n",
    "        stemmed = stemmer.stem(token)\n",
    "        doc_ids = set()\n",
    "        if stemmed in lexicon:\n",
    "            postings = index.getInvertedIndex().getPostings(lexicon[stemmed])\n",
    "            doc_ids = {posting.getId() for posting in postings}\n",
    "        if operator == 'NOT':\n",
    "            all_docs = set(range(len(df)))\n",
    "            doc_ids = all_docs - doc_ids\n",
    "            operator = 'AND'\n",
    "        doc_sets.append((doc_ids, operator))\n",
    "        current_term = stemmed\n",
    "\n",
    "    if not doc_sets:\n",
    "        print('No valid terms found.')\n",
    "        return []\n",
    "\n",
    "    result_docs = doc_sets[0][0]\n",
    "    for i in range(1, len(doc_sets)):\n",
    "        docs, op = doc_sets[i]\n",
    "        if op == 'AND':\n",
    "            result_docs &= docs\n",
    "        elif op == 'OR':\n",
    "            result_docs |= docs\n",
    "\n",
    "    documents = [{'doc_id': doc_id, 'docno': df['docno'].iloc[doc_id], 'title': df['Title'].iloc[doc_id], 'processed_text': df['Processed_Text'].iloc[doc_id]} for doc_id in result_docs]\n",
    "    query_tokens = [stemmer.stem(t) for t in re.findall(r'\\b\\w+\\b', query)]\n",
    "    ranked_docs = rank_documents(documents, query_tokens)\n",
    "\n",
    "    print(f'Boolean search results for \"{query}\":')\n",
    "    for i, doc in enumerate(ranked_docs[:top_k], 1):\n",
    "        print(f'{i}. Docno: {doc[\"docno\"]}, Title: {doc[\"title\"]}, TF-IDF Score: {doc[\"tfidf_score\"]:.4f}')\n",
    "    return ranked_docs\n",
    "\n",
    "boolean_search('aerodynamics AND wing NOT supersonic',df)\n",
    "boolean_search('boundary OR layer',df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e16a2b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Searching for: \"experimental investigation\" ===\n",
      "Query tokens: ['experiment', 'investig']\n",
      "Term 'experiment' not found in index.\n",
      "No documents found.\n",
      "\n",
      "=== Searching for: \"information retrieval\" ===\n",
      "Query tokens: ['inform', 'retriev']\n",
      "Term 'retriev' not found in index.\n",
      "No documents found.\n",
      "\n",
      "=== Searching for: \"nonexistent term\" ===\n",
      "Query tokens: ['nonexist', 'term']\n",
      "Term 'nonexist' not found in index.\n",
      "No documents found.\n"
     ]
    }
   ],
   "source": [
    "def search(query, df, top_k=5):\n",
    "    # index_path = os.path.abspath(\"./CranfieldTitleIndex\")\n",
    "    # if not os.path.exists(index_path) or not os.listdir(index_path):\n",
    "    #     print(\"Index not found. Run Regular_indexing() first.\")\n",
    "    #     return\n",
    "\n",
    "    # index = pt.IndexFactory.of(index_path)\n",
    "    \n",
    "    print(f'\\n=== Searching for: \"{query}\" ===')\n",
    "    query_tokens = preprocess_query(query)\n",
    "    print('Query tokens:', query_tokens)\n",
    "\n",
    "    documents = retrieve_documents(query_tokens, df)\n",
    "    if not documents:\n",
    "        print('No documents found.')\n",
    "        return\n",
    "    print(f'Found {len(documents)} documents.')\n",
    "\n",
    "    ranked_docs = rank_documents(documents, query_tokens)\n",
    "\n",
    "    print(f'Top {min(top_k, len(ranked_docs))} results:')\n",
    "    for i, doc in enumerate(ranked_docs[:top_k], 1):\n",
    "        print(f'{i}. Docno: {doc[\"docno\"]}, Title: {doc[\"title\"]}, TF-IDF Score: {doc[\"tfidf_score\"]:.4f}')\n",
    "\n",
    "search('experimental investigation', df)\n",
    "search('information retrieval', df)\n",
    "search('nonexistent term', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8d53f7",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac5064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"=== Step 1: Choose the Searching method ===\")\n",
    "    print(\"1. Regular Indexing + Search\")\n",
    "    print(\"2. Boolean Retreivel\")\n",
    "    print(\"3.TF-IDF Retreiveal\")\n",
    "\n",
    "    option = int(input(\"Enter the number of the search: \"))\n",
    "    \n",
    "    if option == 1:\n",
    "        Regular_indexing()\n",
    "        query = input(\"Enter your query: \")\n",
    "        # query_terms=preprocess_query(query)\n",
    "        # search_term_regular_indexing(query_terms)\n",
    "        # print(\"================Retreving the Query=================\")\n",
    "        # retrieve_documents(query_terms,df)\n",
    "        search(query,df,top_k=5)\n",
    "    if option == 2 :\n",
    "\n",
    "        query = input(\"Enter your the words: \").strip().lower().split()\n",
    "        operator=input(\"Enter the Operator {AND,OR,NOT} : \")\n",
    "        print(boolean_query(query, operator.upper()))\n",
    "        for\n",
    "    if option ==3 : \n",
    "        query=input(\"Enter your the words: \")\n",
    "        search(query,top_k=5)\n",
    "           \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9fa1294e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 1: Choose the Searching method ===\n",
      "1. Regular Indexing + Search\n",
      "2. Boolean Retreivel\n",
      "3.TF-IDF Retreiveal\n",
      "[1, 84, 189, 372, 423, 497, 569, 662, 766, 816, 836, 858, 1062, 1074, 1075, 1098, 1156, 1159, 1364]\n"
     ]
    }
   ],
   "source": [
    "if main==main() :\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
