{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "project_intro",
      "metadata": {
        "id": "project_intro"
      },
      "source": [
        "# Data Mining Project: Phase 1 and Phase 2\n",
        "\n",
        "our data mining project focused on building a search engine using the Cranfield dataset. Phase 1 involves preprocessing and indexing the dataset, while Phase 2 implements query processing with TF-IDF ranking."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "phase_1_intro",
      "metadata": {
        "id": "phase_1_intro"
      },
      "source": [
        "## Phase 1: Preprocessing and Indexing\n",
        "\n",
        "This section contains Phase 1 implementation, which loads the Cranfield dataset, preprocesses the titles (cleaning, tokenization, stemming), creates an inverted index using PyTerrier, and provides a basic search function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "8t3D_oyXcxVf",
      "metadata": {
        "id": "8t3D_oyXcxVf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "W4vf7dm1ecdR",
      "metadata": {
        "id": "W4vf7dm1ecdR"
      },
      "outputs": [],
      "source": [
        "input_file = \"D:\\DownLoad\\projects\\Search Engine\\Olivia_Searchengine\\datacollection\\output\\cran.all.1400.csv\"\n",
        "output_file = \"D:\\DownLoad\\projects\\Search Engine\\Olivia_Searchengine\\datacollection\\output\\cran_preprocessed_modern.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "0S6unnGueh3S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S6unnGueh3S",
        "outputId": "4285d1f4-74f7-4f14-db03-9edbb67b7508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Loading the Cranfield Dataset ===\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1400 entries, 0 to 1399\n",
            "Data columns (total 4 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Doc_NO  1400 non-null   int64 \n",
            " 1   Title   1398 non-null   object\n",
            " 2   Bib     1330 non-null   object\n",
            " 3   Text    1398 non-null   object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 43.9+ KB\n",
            "None\n",
            "\n",
            "First 5 rows of raw data:\n",
            "   Doc_NO                                              Title  \\\n",
            "0       1  experimental investigation of the aerodynamics...   \n",
            "1       2  simple shear flow past a flat plate in an inco...   \n",
            "2       3  the boundary layer in simple shear flow past a...   \n",
            "3       4  approximate solutions of the incompressible la...   \n",
            "4       5  one-dimensional transient heat conduction into...   \n",
            "\n",
            "                                                 Bib  \\\n",
            "0                         j. ae. scs. 25, 1958, 324.   \n",
            "1  department of aeronautical engineering, rensse...   \n",
            "2  department of mathematics, university of manch...   \n",
            "3                         j. ae. scs. 22, 1955, 728.   \n",
            "4                         j. ae. scs. 24, 1957, 924.   \n",
            "\n",
            "                                                Text  \n",
            "0  experimental investigation of the aerodynamics...  \n",
            "1  simple shear flow past a flat plate in an inco...  \n",
            "2  the boundary layer in simple shear flow past a...  \n",
            "3  approximate solutions of the incompressible la...  \n",
            "4  one-dimensional transient heat conduction into...  \n"
          ]
        }
      ],
      "source": [
        "print(\"=== Loading the Cranfield Dataset ===\")\n",
        "data = pd.read_csv(input_file)\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Dataset Info:\")\n",
        "print(df.info())\n",
        "print(\"\\nFirst 5 rows of raw data:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "y_j6CerUejeG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_j6CerUejeG",
        "outputId": "99290e95-6982-4053-ef73-cda766045eca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Checking for Missing Values ===\n",
            "Missing values in 'Title': 2\n",
            "Missing values in 'Text': 2\n",
            "Total rows before dropping NaN: 1400\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Checking for Missing Values ===\")\n",
        "print(\"Missing values in 'Title':\", df['Title'].isna().sum())\n",
        "print(\"Missing values in 'Text':\", df['Text'].isna().sum())\n",
        "print(\"Total rows before dropping NaN:\", len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "B8rJWu3flEus",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8rJWu3flEus",
        "outputId": "70e71535-a451-49ba-9826-28b00a88f700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total rows after dropping NaN in Title: 1398\n",
            "\n",
            "First 5 rows after dropping NaN:\n",
            "   Doc_NO                                              Title  \\\n",
            "0       1  experimental investigation of the aerodynamics...   \n",
            "1       2  simple shear flow past a flat plate in an inco...   \n",
            "2       3  the boundary layer in simple shear flow past a...   \n",
            "3       4  approximate solutions of the incompressible la...   \n",
            "4       5  one-dimensional transient heat conduction into...   \n",
            "\n",
            "                                                 Bib  \\\n",
            "0                         j. ae. scs. 25, 1958, 324.   \n",
            "1  department of aeronautical engineering, rensse...   \n",
            "2  department of mathematics, university of manch...   \n",
            "3                         j. ae. scs. 22, 1955, 728.   \n",
            "4                         j. ae. scs. 24, 1957, 924.   \n",
            "\n",
            "                                                Text  \n",
            "0  experimental investigation of the aerodynamics...  \n",
            "1  simple shear flow past a flat plate in an inco...  \n",
            "2  the boundary layer in simple shear flow past a...  \n",
            "3  approximate solutions of the incompressible la...  \n",
            "4  one-dimensional transient heat conduction into...  \n"
          ]
        }
      ],
      "source": [
        "df = df.dropna(subset=['Title'])\n",
        "print(\"Total rows after dropping NaN in Title:\", len(df))\n",
        "print(\"\\nFirst 5 rows after dropping NaN:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "BpLbfv4gelRG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpLbfv4gelRG",
        "outputId": "4c4630a1-2192-45c7-e5a2-5f91f5b69484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Step 1: Cleaning Titles ===\n",
            "Sample of cleaned Titles (first 2 rows):\n",
            "   Doc_NO                                      Cleaned_Title\n",
            "0       1  experimental investigation of the aerodynamics...\n",
            "1       2  simple shear flow past a flat plate in an inco...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Step 1: Cleaning Titles ===\")\n",
        "cleaned_titles = []\n",
        "for title in df['Title']:\n",
        "\n",
        "    title_clean = re.sub(r'[^a-zA-Z\\s]', '', str(title))\n",
        "    title_clean = re.sub(r'\\s+', ' ', title_clean).strip()\n",
        "    cleaned_titles.append(title_clean.lower())\n",
        "df['Cleaned_Title'] = cleaned_titles\n",
        "print(\"Sample of cleaned Titles (first 2 rows):\")\n",
        "print(df[['Doc_NO', 'Cleaned_Title']].head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "oMbMCVyLfAXv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMbMCVyLfAXv",
        "outputId": "e329a18f-b55d-4336-fa52-4c1a1fbc82c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Step 2: Tokenizing Titles and Vocabulary Analysis ===\n",
            "Total unique terms in Titles: 1804\n",
            "First 20 terms in Title vocabulary: ['ablating' 'ablation' 'accelerated' 'accelerating' 'according'\n",
            " 'accumulation' 'accuracy' 'acoustic' 'acoustical' 'acting' 'action'\n",
            " 'active' 'adapted' 'addendum' 'addition' 'adiabatic' 'adiabaticwall'\n",
            " 'adjacent' 'advances' 'advancing']\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Step 2: Tokenizing Titles and Vocabulary Analysis ===\")\n",
        "vectorizer = CountVectorizer(\n",
        "    stop_words=\"english\",\n",
        "    lowercase=True,\n",
        "    token_pattern=r'\\b[a-zA-Z]+\\b'\n",
        ")\n",
        "vector = vectorizer.fit_transform(df['Cleaned_Title'])\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "print(\"Total unique terms in Titles:\", len(terms))\n",
        "print(\"First 20 terms in Title vocabulary:\", terms[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "S0vlxQ-RfDVp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0vlxQ-RfDVp",
        "outputId": "69e33a4e-6fe0-42e7-986d-c5d656f54ee6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample tokenized Titles (first 2 rows):\n",
            "   Doc_NO                                       Title_Tokens\n",
            "0       1  [experimental, investigation, of, the, aerodyn...\n",
            "1       2  [simple, shear, flow, past, a, flat, plate, in...\n"
          ]
        }
      ],
      "source": [
        "tokenized_titles = []\n",
        "for title in df['Cleaned_Title']:\n",
        "    words = title.split()\n",
        "    tokenized_titles.append(words)\n",
        "df['Title_Tokens'] = tokenized_titles\n",
        "print(\"\\nSample tokenized Titles (first 2 rows):\")\n",
        "print(df[['Doc_NO', 'Title_Tokens']].head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "uMBGNCk4nH2t",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMBGNCk4nH2t",
        "outputId": "db630e3a-1329-4554-8b5d-0b2d9fd6c9de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Step 3: Comparing Stemming Methods ===\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Step 3: Comparing Stemming Methods ===\")\n",
        "porter = PorterStemmer()\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "lancaster = LancasterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "ruCZVDS5fGla",
      "metadata": {
        "id": "ruCZVDS5fGla"
      },
      "outputs": [],
      "source": [
        "porter_stemmed = []\n",
        "snowball_stemmed = []\n",
        "lancaster_stemmed = []\n",
        "for word in terms:\n",
        "    porter_stemmed.append(porter.stem(word))\n",
        "    snowball_stemmed.append(snowball.stem(word))\n",
        "    lancaster_stemmed.append(lancaster.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "kh4gOYRyi_YG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh4gOYRyi_YG",
        "outputId": "2bd1668e-0ec4-4fd2-bc72-fdb8db4ee7df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Stemming Comparison (First 5 Title Terms):\n",
            "------------------------------------------------------------\n",
            "Original        | Porter          | Snowball        | Lancaster      \n",
            "------------------------------------------------------------\n",
            "ablating        | ablat           | ablat           | abl            \n",
            "ablation        | ablat           | ablat           | abl            \n",
            "accelerated     | acceler         | acceler         | accel          \n",
            "accelerating    | acceler         | acceler         | accel          \n",
            "according       | accord          | accord          | accord         \n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nStemming Comparison (First 5 Title Terms):\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Original':<15} | {'Porter':<15} | {'Snowball':<15} | {'Lancaster':<15}\")\n",
        "print(\"-\" * 60)\n",
        "for i in range(min(5, len(terms))):\n",
        "    print(f\"{terms[i]:<15} | {porter_stemmed[i]:<15} | {snowball_stemmed[i]:<15} | {lancaster_stemmed[i]:<15}\")\n",
        "print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "id": "8Wawr023l1XQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wawr023l1XQ",
        "outputId": "ea484912-b2f2-48c5-e1ac-9b58e0f25094"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Applying Snowball Stemming to Title Tokens...\n",
            "Sample stemmed Titles (first 2 rows):\n",
            "   Doc_NO                               Stemmed_Title_Tokens\n",
            "0       1  [experiment, investig, of, the, aerodynam, of,...\n",
            "1       2  [simpl, shear, flow, past, a, flat, plate, in,...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nApplying Snowball Stemming to Title Tokens...\")\n",
        "stemmed_titles = []\n",
        "for tokens in df['Title_Tokens']:\n",
        "    stemmed_words = []\n",
        "    for word in tokens:\n",
        "        stemmed_words.append(snowball.stem(word))\n",
        "    stemmed_titles.append(stemmed_words)\n",
        "df['Stemmed_Title_Tokens'] = stemmed_titles\n",
        "print(\"Sample stemmed Titles (first 2 rows):\")\n",
        "print(df[['Doc_NO', 'Stemmed_Title_Tokens']].head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "52VX7toJnS8k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52VX7toJnS8k",
        "outputId": "8511f677-835c-46cb-9c94-235d2395a3ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Step 4: Creating Processed_Text from Titles for Indexing ===\n",
            "Sample Processed_Text from Titles (first 2 rows):\n",
            "   Doc_NO                                     Processed_Text\n",
            "0       1  experiment investig of the aerodynam of a wing...\n",
            "1       2  simpl shear flow past a flat plate in an incom...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Step 4: Creating Processed_Text from Titles for Indexing ===\")\n",
        "processed_text = []\n",
        "for stemmed_tokens in df['Stemmed_Title_Tokens']:\n",
        "    joined = \" \".join(stemmed_tokens)\n",
        "    processed_text.append(joined)\n",
        "df['Processed_Text'] = processed_text\n",
        "print(\"Sample Processed_Text from Titles (first 2 rows):\")\n",
        "print(df[['Doc_NO', 'Processed_Text']].head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "KjsrbwbPnXtm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjsrbwbPnXtm",
        "outputId": "066d4b5b-48f8-43c0-ef1d-4cc8f4b3ae4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Step 6: Saving Processed Data ===\n",
            "Saved to: D:\\DownLoad\\projects\\Search Engine\\Olivia_Searchengine\\datacollection\\output\\cran_preprocessed_modern.csv\n",
            "Final output (first 5 rows):\n",
            "   Doc_NO                                              Title  \\\n",
            "0       1  experimental investigation of the aerodynamics...   \n",
            "1       2  simple shear flow past a flat plate in an inco...   \n",
            "2       3  the boundary layer in simple shear flow past a...   \n",
            "3       4  approximate solutions of the incompressible la...   \n",
            "4       5  one-dimensional transient heat conduction into...   \n",
            "\n",
            "                                                 Bib  \\\n",
            "0                         j. ae. scs. 25, 1958, 324.   \n",
            "1  department of aeronautical engineering, rensse...   \n",
            "2  department of mathematics, university of manch...   \n",
            "3                         j. ae. scs. 22, 1955, 728.   \n",
            "4                         j. ae. scs. 24, 1957, 924.   \n",
            "\n",
            "                                                Text  \\\n",
            "0  experimental investigation of the aerodynamics...   \n",
            "1  simple shear flow past a flat plate in an inco...   \n",
            "2  the boundary layer in simple shear flow past a...   \n",
            "3  approximate solutions of the incompressible la...   \n",
            "4  one-dimensional transient heat conduction into...   \n",
            "\n",
            "                                      Processed_Text  \n",
            "0  experiment investig of the aerodynam of a wing...  \n",
            "1  simpl shear flow past a flat plate in an incom...  \n",
            "2  the boundari layer in simpl shear flow past a ...  \n",
            "3  approxim solut of the incompress laminar bound...  \n",
            "4  onedimension transient heat conduct into a dou...  \n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Step 6: Saving Processed Data ===\")\n",
        "output_df = df[['Doc_NO', 'Title', 'Bib', 'Text', 'Processed_Text']]\n",
        "output_df.to_csv(output_file, index=False)\n",
        "print(\"Saved to:\", output_file)\n",
        "print(\"Final output (first 5 rows):\")\n",
        "print(output_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "vw4dIjd8nVnu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw4dIjd8nVnu",
        "outputId": "d9a0749b-9ff3-4e40-c8b5-7398814456f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Step 5: Creative Title Insights ===\n",
            "Average token count per Title: 11.4\n",
            "Longest Title (tokens): 40 in Doc_NO: 1082\n",
            "Most frequent term in Titles (before stemming):\n",
            "'flow' appears 322 times\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Step 5: Creative Title Insights ===\")\n",
        "print(\"Average token count per Title:\", round(df['Title_Tokens'].apply(len).mean(), 2))\n",
        "print(\"Longest Title (tokens):\", df['Title_Tokens'].apply(len).max(), \"in Doc_NO:\",\n",
        "      df['Doc_NO'][df['Title_Tokens'].apply(len).idxmax()])\n",
        "print(\"Most frequent term in Titles (before stemming):\")\n",
        "word_counts = vector.toarray().sum(axis=0)\n",
        "top_term_idx = word_counts.argmax()\n",
        "print(f\"'{terms[top_term_idx]}' appears {word_counts[top_term_idx]} times\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "nCN4_g7KflOl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCN4_g7KflOl",
        "outputId": "31f5496f-70f7-4943-f80c-d8bd5ab330cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-terrier in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (0.13.0)\n",
            "Requirement already satisfied: numpy in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from python-terrier) (2.2.3)\n",
            "Requirement already satisfied: pandas in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from python-terrier) (2.2.3)\n",
            "Requirement already satisfied: more-itertools in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from python-terrier) (10.6.0)\n",
            "Requirement already satisfied: tqdm in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from python-terrier) (4.67.1)\n",
            "Requirement already satisfied: requests in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from python-terrier) (2.32.3)\n",
            "Requirement already satisfied: ir-datasets>=0.3.2 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from python-terrier) (0.5.10)\n",
            "Requirement already satisfied: wget in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from python-terrier) (3.2)\n",
            "Requirement already satisfied: pyjnius>=1.4.2 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from python-terrier) (1.6.1)\n",
            "Requirement already satisfied: deprecated in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from python-terrier) (1.2.18)\n",
            "Requirement already satisfied: scipy in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from python-terrier) (1.15.2)\n",
            "Requirement already satisfied: ir-measures>=0.3.1 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from python-terrier) (0.3.7)\n",
            "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from python-terrier) (0.5.6)\n",
            "Requirement already satisfied: jinja2 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from python-terrier) (3.1.6)\n",
            "Requirement already satisfied: statsmodels in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from python-terrier) (0.14.4)\n",
            "Requirement already satisfied: dill in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from python-terrier) (0.3.9)\n",
            "Requirement already satisfied: joblib in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from python-terrier) (1.4.2)\n",
            "Requirement already satisfied: chest in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from python-terrier) (0.2.3)\n",
            "Requirement already satisfied: lz4 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from python-terrier) (4.4.3)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier) (4.13.3)\n",
            "Requirement already satisfied: inscriptis>=2.2.0 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier) (2.5.3)\n",
            "Requirement already satisfied: lxml>=4.5.2 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier) (5.3.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier) (6.0.2)\n",
            "Requirement already satisfied: trec-car-tools>=2.5.4 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier) (2.6)\n",
            "Requirement already satisfied: warc3-wet>=0.2.3 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier) (0.2.5)\n",
            "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier) (0.2.5)\n",
            "Requirement already satisfied: zlib-state>=0.1.3 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier) (0.1.9)\n",
            "Requirement already satisfied: ijson>=3.1.3 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier) (3.3.0)\n",
            "Requirement already satisfied: unlzw3>=0.2.1 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier) (0.2.3)\n",
            "Requirement already satisfied: pyarrow>=16.1.0 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier) (19.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from requests->python-terrier) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from requests->python-terrier) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from requests->python-terrier) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from requests->python-terrier) (2025.1.31)\n",
            "Requirement already satisfied: colorama in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from tqdm->python-terrier) (0.4.6)\n",
            "Requirement already satisfied: heapdict in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from chest->python-terrier) (1.0.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from deprecated->python-terrier) (1.17.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from jinja2->python-terrier) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from pandas->python-terrier) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from pandas->python-terrier) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from pandas->python-terrier) (2025.1)\n",
            "Requirement already satisfied: patsy>=0.5.6 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from statsmodels->python-terrier) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from statsmodels->python-terrier) (24.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from beautifulsoup4>=4.4.1->ir-datasets>=0.3.2->python-terrier) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from beautifulsoup4>=4.4.1->ir-datasets>=0.3.2->python-terrier) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->python-terrier) (1.17.0)\n",
            "Requirement already satisfied: cbor>=1.0.0 in d:\\download\\projects\\search engine\\olivia_searchengine\\venv\\lib\\site-packages (from trec-car-tools>=2.5.4->ir-datasets>=0.3.2->python-terrier) (1.0.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install python-terrier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "1r2h34lcfoKl",
      "metadata": {
        "id": "1r2h34lcfoKl"
      },
      "outputs": [],
      "source": [
        "import pyterrier as pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "Zrns3BJMfue4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zrns3BJMfue4",
        "outputId": "5fe7e252-b1ed-4f6b-9695-a38e7a4d122d"
      },
      "outputs": [],
      "source": [
        "if not pt.java.started():\n",
        "    pt.java.init()\n",
        "    print(\"Java Virtual Machine started!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "id": "WN011XDZfx1a",
      "metadata": {
        "id": "WN011XDZfx1a"
      },
      "outputs": [],
      "source": [
        "input_file = \"D:\\DownLoad\\projects\\Search Engine\\Olivia_Searchengine\\datacollection\\output\\cran_preprocessed_modern.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "dC4N9G_qo2kg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC4N9G_qo2kg",
        "outputId": "60e12b5e-cd7d-46a7-94bb-e48b39b4a532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Doc_NO                                              Title  \\\n",
            "0       1  experimental investigation of the aerodynamics...   \n",
            "1       2  simple shear flow past a flat plate in an inco...   \n",
            "2       3  the boundary layer in simple shear flow past a...   \n",
            "3       4  approximate solutions of the incompressible la...   \n",
            "4       5  one-dimensional transient heat conduction into...   \n",
            "\n",
            "                                                 Bib  \\\n",
            "0                         j. ae. scs. 25, 1958, 324.   \n",
            "1  department of aeronautical engineering, rensse...   \n",
            "2  department of mathematics, university of manch...   \n",
            "3                         j. ae. scs. 22, 1955, 728.   \n",
            "4                         j. ae. scs. 24, 1957, 924.   \n",
            "\n",
            "                                                Text  \\\n",
            "0  experimental investigation of the aerodynamics...   \n",
            "1  simple shear flow past a flat plate in an inco...   \n",
            "2  the boundary layer in simple shear flow past a...   \n",
            "3  approximate solutions of the incompressible la...   \n",
            "4  one-dimensional transient heat conduction into...   \n",
            "\n",
            "                                      Processed_Text  \n",
            "0  experiment investig of the aerodynam of a wing...  \n",
            "1  simpl shear flow past a flat plate in an incom...  \n",
            "2  the boundari layer in simpl shear flow past a ...  \n",
            "3  approxim solut of the incompress laminar bound...  \n",
            "4  onedimension transient heat conduct into a dou...  \n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(input_file)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "nzKiw_9ypAbD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzKiw_9ypAbD",
        "outputId": "18a1b09a-fbdd-4c56-84f5-1871f1661926"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample with docno (first 2 rows):\n",
            "  docno                                              Title  \\\n",
            "0     1  experimental investigation of the aerodynamics...   \n",
            "1     2  simple shear flow past a flat plate in an inco...   \n",
            "\n",
            "                                      Processed_Text  \n",
            "0  experiment investig of the aerodynam of a wing...  \n",
            "1  simpl shear flow past a flat plate in an incom...  \n"
          ]
        }
      ],
      "source": [
        "df[\"docno\"] = df[\"Doc_NO\"].astype(str)\n",
        "print(\"\\nSample with docno (first 2 rows):\")\n",
        "print(df[['docno', 'Title', 'Processed_Text']].head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "qrdFGYFapFQ7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrdFGYFapFQ7",
        "outputId": "687d3c06-f0c4-4c29-c0f9-f257e316f56d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Step 1: Creating and Indexing the Titles ===\n",
            "23:09:06.464 [main] ERROR org.terrier.structures.indexing.Indexer -- Could not rename index\n",
            "java.io.IOException: Rename of index structure file 'd:\\DownLoad\\projects\\Search Engine\\Olivia_Searchengine\\preprocessing\\CranfieldTitleIndex/data_1.direct.bf' (exists) to 'd:\\DownLoad\\projects\\Search Engine\\Olivia_Searchengine\\preprocessing\\CranfieldTitleIndex/data.direct.bf' (exists) failed - likely that source file is still open. Possible indexing bug?\n",
            "\tat org.terrier.structures.IndexUtil.renameIndex(IndexUtil.java:379)\n",
            "\tat org.terrier.structures.indexing.Indexer.index(Indexer.java:388)\n",
            "\tat org.terrier.structures.indexing.Indexer.index(Indexer.java:355)\n",
            "Index location: d:\\DownLoad\\projects\\Search Engine\\Olivia_Searchengine\\preprocessing\\CranfieldTitleIndex/data.properties\n",
            "Indexing complete! Stored at: d:\\DownLoad\\projects\\Search Engine\\Olivia_Searchengine\\preprocessing\\CranfieldTitleIndex/data.properties\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print(\"\\n=== Step 1: Creating and Indexing the Titles ===\")\n",
        "\n",
        "index_path = os.path.abspath(\"./CranfieldTitleIndex\")\n",
        "if not os.path.exists(index_path):\n",
        "\tos.makedirs(index_path, exist_ok=True)\n",
        "\n",
        "indexer = pt.DFIndexer(index_path, overwrite=True)\n",
        "index_ref = indexer.index(df[\"Processed_Text\"], df[\"docno\"])\n",
        "print(\"Index location:\", index_ref.toString())\n",
        "print(\"Indexing complete! Stored at:\", index_ref.toString())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "dUi1YzJwpdyp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUi1YzJwpdyp",
        "outputId": "8545b6b0-c4a2-4c75-f8bd-4da10548f107"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Step 2: Loading the Index ===\n",
            "Index loaded successfully!\n",
            "<org.terrier.structures.Index at 0x1506f856a30 jclass=org/terrier/structures/Index jself=<LocalRef obj=0x6c957dc2 at 0x1502cdbe810>>\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Step 2: Loading the Index ===\")\n",
        "\n",
        "index = pt.IndexFactory.of(index_ref)\n",
        "print(\"Index loaded successfully!\")\n",
        "print(index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "3-KXC9Kopg2w",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-KXC9Kopg2w",
        "outputId": "49db218e-2bf8-45c9-e0ad-9753d466593e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ablat -> Nt=12 TF=12 maxTF=1\n",
            "accel -> Nt=2 TF=2 maxTF=1\n",
            "accord -> Nt=1 TF=1 maxTF=1\n",
            "accumul -> Nt=1 TF=1 maxTF=1\n",
            "accuraci -> Nt=2 TF=2 maxTF=1\n",
            "acoust -> Nt=5 TF=5 maxTF=1\n",
            "act -> Nt=1 TF=1 maxTF=1\n",
            "action -> Nt=1 TF=1 maxTF=1\n",
            "activ -> Nt=1 TF=1 maxTF=1\n",
            "adapt -> Nt=1 TF=1 maxTF=1\n"
          ]
        }
      ],
      "source": [
        "lexicon = index.getLexicon()\n",
        "\n",
        "count = 0\n",
        "\n",
        "\n",
        "for kv in lexicon:\n",
        "    if count < 10:\n",
        "        term = kv.getKey()\n",
        "        entry = kv.getValue()\n",
        "        print(f\"{term} -> Nt={entry.getNumberOfEntries()} TF={entry.getFrequency()} maxTF={entry.getMaxFrequencyInDocuments()}\")\n",
        "        count = count + 1\n",
        "    else:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "WJKt0WmLqA3V",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJKt0WmLqA3V",
        "outputId": "57ed0fc0-3daf-4e1c-fe20-37d769328d7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Step 5: Setting Up Search Function ===\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Step 5: Setting Up Search Function ===\")\n",
        "def search_term(term):\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    term = term.lower()\n",
        "    stemmed_term = stemmer.stem(term)\n",
        "\n",
        "    print(f\"\\nSearching for: '{term}' (stemmed: '{stemmed_term}')\")\n",
        "\n",
        "    try:\n",
        "        pointer = index.getLexicon()[stemmed_term]\n",
        "        print(f\"Found term '{stemmed_term}' with stats: {pointer.toString()}\")\n",
        "        print(\"Documents containing the term:\")\n",
        "        postings = index.getInvertedIndex().getPostings(pointer)\n",
        "\n",
        "\n",
        "        for posting in postings:\n",
        "            doc_id = posting.getId()\n",
        "            doc_length = posting.getDocumentLength()\n",
        "            print(f\"- Doc ID: {doc_id} (docno: {df['docno'].iloc[doc_id]}), Length: {doc_length}\")\n",
        "    except KeyError:\n",
        "        print(f\"Term '{stemmed_term}' not found in the index.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "oXcg_IEGrIKx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXcg_IEGrIKx",
        "outputId": "a3aed864-65d3-4e7e-eef9-e3706680286c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Searching for: 'information' (stemmed: 'inform')\n",
            "Found term 'inform' with stats: term700 Nt=1 TF=1 maxTF=1 @{0 5628 7}\n",
            "Documents containing the term:\n",
            "- Doc ID: 439 (docno: 440), Length: 8\n",
            "\n",
            "Searching for: 'omar' (stemmed: 'omar')\n",
            "Term 'omar' not found in the index.\n"
          ]
        }
      ],
      "source": [
        "search_term(\"information\")\n",
        "search_term(\"Omar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "phase_2_intro",
      "metadata": {
        "id": "phase_2_intro"
      },
      "source": [
        "## Phase 2: Query Processing with TF-IDF Ranking\n",
        "\n",
        "This section implements Phase 2 of the project, focusing on query processing with expanded capabilities. It includes parsing user queries, applying the same preprocessing steps as in Phase 1 (tokenization, lowercase, stemming), retrieving documents containing all query terms using the inverted index, and ranking them using TF-IDF scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "phase_2_import_libraries",
      "metadata": {
        "id": "phase_2_import_libraries"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "preprocess_query",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "preprocess_query",
        "outputId": "2cfb3345-f43b-49f4-8dc6-6eaa5f7d1317"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample query: Experimental Aerodynamics Wing\n",
            "Preprocessed query tokens: ['experiment', 'aerodynam', 'wing']\n"
          ]
        }
      ],
      "source": [
        "def preprocess_query(query, stemmer=SnowballStemmer('english')):\n",
        "    query = query.lower()\n",
        "    query = re.sub(r'[^a-zA-Z\\s]', '', query)\n",
        "    query = re.sub(r'\\s+', ' ', query).strip()\n",
        "    tokens = query.split()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return stemmed_tokens\n",
        "\n",
        "\n",
        "sample_query = 'Experimental Aerodynamics Wing'\n",
        "print('Sample query:', sample_query)\n",
        "print('Preprocessed query tokens:', preprocess_query(sample_query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "retrieve_documents",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "retrieve_documents",
        "outputId": "8a9471b0-2784-4056-9c0d-926e2db557ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Term 'experiment' not found in index.\n",
            "\n",
            "Documents retrieved for query \"experimental investigation\":\n"
          ]
        }
      ],
      "source": [
        "def retrieve_documents(query_tokens, index, df):\n",
        "    lexicon = index.getLexicon()\n",
        "    doc_sets = []\n",
        "\n",
        "    for token in query_tokens:\n",
        "        try:\n",
        "            pointer = lexicon[token]\n",
        "            postings = index.getInvertedIndex().getPostings(pointer)\n",
        "            doc_ids = [posting.getId() for posting in postings]\n",
        "            doc_sets.append(set(doc_ids))\n",
        "        except KeyError:\n",
        "            print(f\"Term '{token}' not found in index.\")\n",
        "            return []\n",
        "\n",
        "    if not doc_sets:\n",
        "        return []\n",
        "    common_docs = list(set.intersection(*doc_sets))\n",
        "\n",
        "    results = []\n",
        "    for doc_id in common_docs:\n",
        "        docno = df['docno'].iloc[doc_id]\n",
        "        title = df['Title'].iloc[doc_id]\n",
        "        processed_text = df['Processed_Text'].iloc[doc_id]\n",
        "        results.append({\n",
        "            'doc_id': doc_id,\n",
        "            'docno': docno,\n",
        "            'title': title,\n",
        "            'processed_text': processed_text\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "test_query = 'experimental investigation'\n",
        "test_tokens = preprocess_query(test_query)\n",
        "docs = retrieve_documents(test_tokens, index, df)\n",
        "print(f'\\nDocuments retrieved for query \"{test_query}\":')\n",
        "for doc in docs[:2]:\n",
        "    print(f\"Docno: {doc['docno']}, Title: {doc['title']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "rank_documents",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rank_documents",
        "outputId": "4a3fc661-6e11-4871-9219-fcd71d086029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top ranked documents for query \"experimental investigation\":\n"
          ]
        }
      ],
      "source": [
        "def rank_documents(documents, query_tokens):\n",
        "    if not documents:\n",
        "        return []\n",
        "\n",
        "    corpus = [doc['processed_text'] for doc in documents]\n",
        "    query = ' '.join(query_tokens)\n",
        "\n",
        "    vectorizer = TfidfVectorizer(vocabulary=query_tokens)\n",
        "    try:\n",
        "        tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "        scores = tfidf_matrix.sum(axis=1).A1\n",
        "    except ValueError as e:\n",
        "        print('TF-IDF calculation failed:', e)\n",
        "        scores = [0] * len(documents)\n",
        "\n",
        "    for i, doc in enumerate(documents):\n",
        "        doc['tfidf_score'] = scores[i]\n",
        "\n",
        "    ranked_docs = sorted(documents, key=lambda x: x['tfidf_score'], reverse=True)\n",
        "    return ranked_docs\n",
        "\n",
        "ranked_docs = rank_documents(docs, test_tokens)\n",
        "print(f'\\nTop ranked documents for query \"{test_query}\":')\n",
        "for doc in ranked_docs[:2]:\n",
        "    print(f\"Docno: {doc['docno']}, Title: {doc['title']}, TF-IDF Score: {doc['tfidf_score']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "search_function",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "search_function",
        "outputId": "08b4aefb-1d53-4ddf-f2f8-37a9860976f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Searching for: \"experimental investigation\" ===\n",
            "Query tokens: ['experiment', 'investig']\n",
            "Term 'experiment' not found in index.\n",
            "No documents found.\n",
            "\n",
            "=== Searching for: \"information retrieval\" ===\n",
            "Query tokens: ['inform', 'retriev']\n",
            "Term 'retriev' not found in index.\n",
            "No documents found.\n",
            "\n",
            "=== Searching for: \"nonexistent term\" ===\n",
            "Query tokens: ['nonexist', 'term']\n",
            "Term 'nonexist' not found in index.\n",
            "No documents found.\n"
          ]
        }
      ],
      "source": [
        "def search(query, index, df, top_k=5):\n",
        "    print(f'\\n=== Searching for: \"{query}\" ===')\n",
        "    query_tokens = preprocess_query(query)\n",
        "    print('Query tokens:', query_tokens)\n",
        "\n",
        "    documents = retrieve_documents(query_tokens, index, df)\n",
        "    if not documents:\n",
        "        print('No documents found.')\n",
        "        return\n",
        "    print(f'Found {len(documents)} documents.')\n",
        "\n",
        "    ranked_docs = rank_documents(documents, query_tokens)\n",
        "\n",
        "    print(f'Top {min(top_k, len(ranked_docs))} results:')\n",
        "    for i, doc in enumerate(ranked_docs[:top_k], 1):\n",
        "        print(f'{i}. Docno: {doc[\"docno\"]}, Title: {doc[\"title\"]}, TF-IDF Score: {doc[\"tfidf_score\"]:.4f}')\n",
        "\n",
        "search('experimental investigation', index, df)\n",
        "search('information retrieval', index, df)\n",
        "search('nonexistent term', index, df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
